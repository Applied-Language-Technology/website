{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating language models\n",
    "\n",
    "This section introduces you to some basic techniques for evaluating the results of natural language processing.\n",
    "\n",
    "After reading this section, you should:\n",
    "\n",
    "- understand what is meant by a gold standard\n",
    "- know how to evaluate agreement between human annotators\n",
    "- understand simple metrics for evaluating the performance of natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a gold standard?\n",
    "\n",
    "A gold standard ‚Äì also called a ground truth ‚Äì refers to human-verified data that can used as a benchmark for evaluating the performance of algorithms. \n",
    "\n",
    "In natural language processing, a gold standard attempts to measure how well humans perform on some task.\n",
    "\n",
    "The goal of natural language processing is to allow computers to achieve or surpass human-level performance in some pre-defined task. \n",
    "\n",
    "Measuring whether they can do so requires a benchmark, which is provided by the gold standard.\n",
    "\n",
    "In reality, however, gold standards consist of *abstractions*. \n",
    "\n",
    "Think of placing words into word classes: word classes are not given to us by nature, but represent an abstraction that imposes structure on natural language.\n",
    "\n",
    "Language, however, is naturally ambiguous and subjective, and the abstractions used be underdeveloped ‚Äì we cannot be sure if we would describe or categorise phenomenona the same way.\n",
    "\n",
    "This is why we need also to measure the reliability of any gold standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring reliability manually\n",
    "\n",
    "This section introduces how reliability, often understood as agreement between multiple annotators, can be measured.\n",
    "\n",
    "### Step 1: Annotate data\n",
    "\n",
    "Sentiment analysis is a task that involves determining the sentiment of a text (for an useful overview that incorporates insights from both linguistics and natural language processing, see [Taboada](https://doi.org/10.1146/annurev-linguistics-011415-040518) (2016).\n",
    "\n",
    "Training a sentiment analysis model requires collecting training data, that is, examples of texts associated with different sentiments.\n",
    "\n",
    "Classify the following tweets into three categories ‚Äì *positive*, *neutral* or *negative* ‚Äì based on their sentiment.\n",
    "\n",
    "Write down your decision ‚Äì one per row ‚Äì but **do not discuss them or show them to the person next to you.**\n",
    "\n",
    "```\n",
    "1. Updated: HSL GTFS (Helsinki, Finland) https://t.co/fWEpzmNQLz\n",
    "2. current weather in Helsinki: broken clouds, -8¬∞C 100% humidity, wind 4kmh, pressure 1061mb\n",
    "3. \"\"suddenly became dark\"\"? The man is confused by daylight and we let him run national infrastructure?\n",
    "4. Baana bicycle counter. Today: 3 Same time last week: 1058 Trend: ‚Üì99% This year: 819 518 Last year: 802 079 #Helsinki #py√∂r√§ily #cycling\n",
    "5. What a great start for 2018! My first title at the 48th Brunswick Ballmaster Open (first‚Ä¶ \n",
    "6. A perfect Sunday walk in the woods just a few steps from home. This is what my beloved hometown,‚Ä¶ \n",
    "7. Went to Drug Restaurant concert todayüëç It was so amazing and I think I got damn good photos with‚Ä¶\n",
    "8. Choo Choo üöÇ train cake for little boys birthday party üéâ - Junakakku üöÇ pikkupojan‚Ä¶ \n",
    "9. Happy women's day ‚ù§Ô∏èüíã kisses to all you beautiful ladies out there. üòö #awesometobeawoman\n",
    "10. Good morning #Helsinki! Sun will rise in 30 minutes (local time 07:32) #Sunrise #Photo #Photography [03.10.2016]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "Double-click this cell to edit its contents and write down your classifications below:\n",
    "\n",
    "    1.\n",
    "    2.\n",
    "    3.\n",
    "    4.\n",
    "    5.\n",
    "    6.\n",
    "    7.\n",
    "    8.\n",
    "    9.\n",
    "    10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate percentage agreement\n",
    "\n",
    "When creating datasets for training models, we typically want the training data to be reliable, that is, so that we agree on whatever we are describing ‚Äì in this case, the sentiment of the tweets above.\n",
    "\n",
    "One way to measure this is simple *percentage agreement*, that is, how many times out of 10 you and the person next to you agreed on the sentiment of a tweet.\n",
    "\n",
    "Now compare your results calculate percentage agreement by dividing the number of times you agreed by the number of items (10).\n",
    "\n",
    "You can calculate percentage agreement by executing the cell below: just assign the number items you agree on to the variable `agreement`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement = 0  # Replace this number here\n",
    "\n",
    "agreement = agreement / 10  # Divide the count by the number of tweets\n",
    "\n",
    "agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate probabilities for each category\n",
    "\n",
    "Percentage agreement is actually a very poor measure of agreement, as either of you may have made lucky guesses ‚Äì or perhaps you considered the task boring and classified every tweet into a random category.\n",
    "\n",
    "If you did, we have no way of knowing this, as percentage agreement cannot tell us if the result occurred by chance!\n",
    "\n",
    "Luckily, we can estimate the possibility of *chance agreement* easily.\n",
    "\n",
    "The first step is to count **how many times you used each available category** (positive, neutral or negative).\n",
    "\n",
    "Assign these counts in the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 0\n",
    "neutral = 0\n",
    "negative = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert these counts into *probabilities* by dividing them with the total number of tweets classified (10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = positive / 10\n",
    "neutral = neutral / 10\n",
    "negative = negative / 10\n",
    "\n",
    "positive, neutral, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These probabilities represent the chance of *you* choosing that particular category.\n",
    "\n",
    "Now ask the person sitting next to you for their corresponding probabilities and tell them yours as well. \n",
    "\n",
    "Add their probabilities to the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_positive = 0\n",
    "nb_neutral = 0\n",
    "nb_negative = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the probabilities for each class for both annotators, we can calculate the probability that both annotators choose the same category by chance.\n",
    "\n",
    "This is easy: for each category, simply multiply your probability with the corresponding probability from the person next to you.\n",
    "\n",
    "If either annotator did not assign a single tweet into a category, e.g. negative, and the other annotator did, then this effectively rules out the possibility of agreeing by chance (multiplication by zero results in zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_positive = positive * nb_positive\n",
    "both_neutral = neutral * nb_neutral\n",
    "both_negative = negative * nb_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Estimate chance agreement\n",
    "\n",
    "Now we are ready to calculate how likely you are to agree by chance.\n",
    "\n",
    "This is known as *expected agreement*, which is calculated by summing up your combined probabilities for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_agreement = both_positive + both_neutral + both_negative\n",
    "\n",
    "expected_agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know both observed percentage agreement (`agreement`) and the agreement expected by chance (`expected_agreement`), we can use this information for a more reliable measure of *agreement*.\n",
    "\n",
    "One such measure is [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa) ($\\kappa$), which estimates agreement on the basis of both observed and expected agreement.\n",
    "\n",
    "The formula for Cohen's $\\kappa$ is as follows:\n",
    "\n",
    "$\\kappa = \\frac{P_{observed} - P_{expected}}{1 - P_{expected}}$\n",
    "\n",
    "As all this information is stored in our variables `agreement` and `expected_agreement`, we can easily count the $\\kappa$ score using the code below.\n",
    "\n",
    "Note that we must wrap the subtractions into parentheses to perform them before division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = (agreement - expected_agreement) / (1 - expected_agreement)\n",
    "\n",
    "kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the result for Cohen's $\\kappa$.\n",
    "\n",
    "Let's now consider how to interpret its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen's $\\kappa$ as a measure of agreement\n",
    "\n",
    "The theoretical value Cohen's $\\kappa$ runs from $-1$ indicating perfect disagreement to $+1$ for perfect agreement, with $0$ standing for completely random agreement.\n",
    "\n",
    "The $\\kappa$ score is often interpreted as a measure of the strength of agreement.\n",
    "\n",
    "[Landis and Koch](https://doi.org/10.2307/2529310) (1977) famously proposed the following benchmarks, which should nevertheless be taken with a pinch of salt as the divisions are completely arbitrary.\n",
    "\n",
    "```\n",
    "| Cohen's K | Strength of agreement|\n",
    "|----------------------------------|\n",
    "| <0.00     | Poor                 |\n",
    "| 0.00‚Äì0.20 | Slight               |\n",
    "| 0.21‚Äì0.40 | Fair                 |\n",
    "| 0.41‚Äì0.60 | Moderate             |\n",
    "| 0.61‚Äì0.80 | Substantial          |\n",
    "| 0.81‚Äì1.00 | Almost perfect       |\n",
    "|----------------------------------|\n",
    "```\n",
    "\n",
    "Cohen's $\\kappa$ can be used to measure agreement between **two** annotators and the categories available must be **fixed** in advance. \n",
    "\n",
    "For measuring agreement between more than two annotators, one must use a measure such as [Fleiss' $\\kappa$](https://en.wikipedia.org/wiki/Fleiss%27_kappa).\n",
    "\n",
    "Cohen's $\\kappa$ and many more measures of agreement are implemented in various Python libraries, so one rarely needs to perform the calculations manually.\n",
    "\n",
    "The [*scikit-learn*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score) library (`sklearn`), for instance, includes an implementation of Cohen's $\\kappa$.\n",
    "\n",
    "Let's import the `cohen_kappa_score()` function for calculating Cohen's $\\kappa$ from scikit-learn.\n",
    "\n",
    "This function takes two *lists* as input and calculates the $\\kappa$ score between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define two lists of part-of-speech tags, which make up our toy example for calculating Cohen's $\\kappa$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = ['ADJ', 'AUX', 'NOUN', 'VERB', 'VERB']\n",
    "a2 = ['ADJ', 'VERB', 'NOUN', 'NOUN', 'VERB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to feed the two lists, `a1` and `a2`, to the `cohen_kappa_score()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44444444444444453"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the benchmark from Landis and Koch, this score would indicate moderate agreement.\n",
    "\n",
    "Generally, Cohen's $\\kappa$ can be used for measuring agreement on all kinds of tasks that involve placing items into categories.\n",
    "\n",
    "It is rarely necessary to annotate the whole dataset when measuring agreement ‚Äì a random sample is often enough.\n",
    "\n",
    "Finally, if Cohen's $\\kappa$ suggests that the human annotators agree on whatever they are categorising, we can assume that the end result is *reliable*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of language models\n",
    "\n",
    "Once we have a sufficiently *reliable* gold standard, we can use the gold standard to measure the performance of language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have a reliable gold standard consisting of 10 tokens annotated for their part-of-speech tags.\n",
    "\n",
    "These part-of-speech tags are given in the list `gold_standard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = ['ADJ', 'ADJ', 'AUX', 'VERB', 'AUX', 'NOUN', 'NOUN', 'ADJ', 'DET', 'PRON']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then retrieve the predictions for the same tokens from some language model and store them in a list named `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ['NOUN', 'ADJ', 'AUX', 'VERB', 'AUX', 'NOUN', 'VERB', 'ADJ', 'DET', 'PROPN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Now that we have two sets of annotations to compare, let's import some metrics from the *scikit-learn* library and apply them to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can calculate [**accuracy**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) using the `accuracy_score()` function, which is precisely the same as observed agreement.\n",
    "\n",
    "This function takes two lists as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(gold_standard, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that accuracy suffers from the same shortcoming as observed agreement ‚Äì we cannot know if the language model is simply making lucky guesses.\n",
    "\n",
    "However, given that we are working with a toy example, we can easily verify that 7 out of 10 part-of-speech tags match. This gives an accuracy of 0.7 or 70%.\n",
    "\n",
    "To better evaluate the performance of the language model against our gold standard, let's organise the results into a *confusion matrix*.\n",
    "\n",
    "To do so, we need all the classes that occur in `gold_standard` and `predictions`.\n",
    "\n",
    "We can easily get a list of unique classes using Python's `set()` function.\n",
    "\n",
    "We then sort the set and cast it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ', 'AUX', 'DET', 'NOUN', 'PRON', 'PROPN', 'VERB']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = set(gold_standard + predictions)\n",
    "\n",
    "pos_tags = list(sorted(pos_tags))\n",
    "\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compile this information into a table, in which the rows stand for the gold standard and the columns stand for predictions made by the language model.\n",
    "\n",
    "This kind of table is commonly called a *confusion matrix*.\n",
    "\n",
    "The table is populated by simply walking through each pair of items in the gold standard and model predictions, adding $+1$ to the cell for this combination.\n",
    "\n",
    "For example, the first item in `gold_standard` is ADJ, whereas the first item in `predictions` is NOUN. \n",
    "\n",
    "We then find the row for ADJ and the column for NOUN and add one to this cell.\n",
    "\n",
    "```\n",
    "                            PREDICTIONS\n",
    "\n",
    "           | ADJ | AUX | DET | NOUN | PRON | PROPN | VERB |\n",
    "   |-------|-----|-----|-----|------|------|-------|------|\n",
    "   | ADJ   | 2   | 0   | 0   | 1    | 0    | 0     | 0    |\n",
    "G  | AUX   | 0   | 2   | 0   | 0    | 0    | 0     | 0    |\n",
    "O  | DET   | 0   | 0   | 1   | 0    | 0    | 0     | 0    |\n",
    "L  | NOUN  | 0   | 0   | 0   | 1    | 0    | 0     | 1    |\n",
    "D  | PRON  | 0   | 0   | 0   | 0    | 0    | 1     | 0    |\n",
    "   | PROPN | 0   | 0   | 0   | 0    | 0    | 0     | 0    |\n",
    "   | VERB  | 0   | 0   | 0   | 0    | 0    | 0     | 1    |\n",
    "   |------------------------------------------------------|\n",
    "```\n",
    "\n",
    "As you can see, the correct predictions form a roughly diagonal line across the table.\n",
    "\n",
    "We can use the table to derive two additional metrics for each class: **precision** and **recall**.\n",
    "\n",
    "Precision is the **proportion of correct predictions per class**. In plain words, precision tells you how many predictions were correct for each class.\n",
    "\n",
    "For example, the sum for column VERB is $2$, of which $1$ prediction is correct (that which is located in the row VERB).\n",
    "\n",
    "Hence precision for VERB is $1 / 2 = 0.5$ ‚Äì half of the tokens predicted to be verbs were classified correctly. The same holds true for NOUN, as the column sums up two $2$, but only $1$ prediction is in the correct row.\n",
    "\n",
    "Recall, in turn, gives the proportion of correct predictions for all examples of that class. \n",
    "\n",
    "Put differently, recall tells you **how many actual instances of a given class the model was able to \"find\"**.\n",
    "\n",
    "For example, the sum for row ADJ is $3$: there are three adjectives in the gold standard, but only two are located in the corresponding column for ADJ.\n",
    "\n",
    "This means that recall for ADJ is $2 / 3 = 0.66$ ‚Äì approximately 66% of the adjectives present in the gold standard were classified correctly. For NOUN, recall is $1 / 2 = 0.5$.\n",
    "\n",
    "The *scikit-learn* library provides a `confusion_matrix()` function for automatically generating confusion matrices. \n",
    "\n",
    "Execute the following cell and compare it to the manually created matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 1 0 0 0]\n",
      " [0 2 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(gold_standard, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the ability of the language model to predict the correct part-of-speech tag, we can use [**precision**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score), which is implemented in the `precision_score()` function in the *scikit-learn* library.\n",
    "\n",
    "Because we have more than two classes for part-of-speech tags instead of just two (binary) classes, we must define how the results for each class are processed.\n",
    "\n",
    "This option is set using the `average` argument of the `precision_score()` function.\n",
    "\n",
    "If we set `average` to `None`, the function returns precision for each class.\n",
    "\n",
    "The results are organised according to a sorted *set* of labels present in `gold_standard` and `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(gold_standard, predictions, average=None)\n",
    "\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *scikit-learn* will raise a *warning*, since not all classes in `predictions` are present in `gold_standard`. Note, however, that this is a warning, not an error.\n",
    "\n",
    "The output is a [NumPy](https://www.numpy.org) array. NumPy is a powerful library for working with numerical data which can be found under the hood of many Python libraries.\n",
    "\n",
    "If we want to combine our list of labels in `pos_tags` with the precision scores in `precision`, we can do this using Python's `zip()` function, which joins together lists and/or arrays of the same size. To view the result, we must cast it into a dictionary using `dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(pos_tags, precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get single precision score for all classes, we can use the option given by the string `'macro'`, which means that each class is treated as equally important regardless of how many examples of this class can be found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_precision = metrics.precision_score(gold_standard, predictions, average='macro')\n",
    "\n",
    "macro_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The macro precision is calculated by summing up the precision scores and dividing them by the number of classes.\n",
    "\n",
    "We can easily verify this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(precision) / len(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating recall is equally easy using the `recall_score()` function from *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = metrics.recall_score(gold_standard, predictions, average=None)\n",
    "\n",
    "dict(zip(pos_tags, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scikit-learn* library provides a very useful function for providing an overview of classification performance called `classification_report()`.\n",
    "\n",
    "This will give you the precision and recall scores for each class, together with the [F1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), which is the weighted average of precision and recall. The column for support counts the number of instances in each class.\n",
    "\n",
    "As you can see, the micro average scores, which compare the gold standard and predictions *globally*, are the same as accuracy.\n",
    "\n",
    "The macro average scores, in turn, correspond to those that we calculated above.\n",
    "\n",
    "Finally, the weighted averages take into account the number of instances in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(gold_standard, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should have given you an idea how to assess the reliability of human annotations, and how reliable annotations can be used as a gold standard for benchmarking the performance of natural language processing."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
