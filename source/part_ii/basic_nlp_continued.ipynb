{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customising spaCy for efficiency\n",
    "\n",
    "This section introduces you to customising the spaCy pipeline, that is, determining just what spaCy does with text and how.\n",
    "\n",
    "After reading this section, you should know how to:\n",
    "\n",
    " - examine and modify the spaCy pipeline\n",
    " - process large collections of texts efficiently \n",
    " - simplify spaCy output\n",
    "\n",
    "Let's start by importing the *spaCy* library and the *displacy* module for drawing dependency trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library and the displacy module\n",
    "from spacy import displacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import a language model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x10bef4610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a small language model for English and assign it to the variable 'nlp'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Call the variable to examine the object\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customising the spaCy pipeline\n",
    "\n",
    "Let's start by examining the spaCy _Language_ object in greater detail.\n",
    "\n",
    "The _Language_ object is a essentially pipeline that applies a language model to text, performing a series of tasks that the model has been trained to do. These tasks depend on the components present in the pipeline.\n",
    "\n",
    "We can examine the components of a pipeline using the `pipeline` attribute of a _Language_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x13ad109a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x13ad21d10>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x13acc4640>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x13ab12be0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x13ad12840>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x13acb27c0>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a spaCy *SimpleFrozenList* object, which consists of Python _tuples_ with two items: \n",
    "\n",
    " 1. component names, e.g. `tagger`, \n",
    " 2. the actual components that perform different tasks, e.g. `spacy.pipeline.tok2vec.Tok2Vec`.\n",
    "\n",
    "Components such as `tagger`, `parser`, `ner`and `lemmatizer` should already be familiar to you from the [previous section](basic_nlp.ipynb).\n",
    "\n",
    "There are, however, two components present in `nlp.pipeline` that we have not yet encountered. \n",
    "\n",
    " - `tok2vec` maps *Tokens* to word vectors, which are numerical representations of the semantic context in which the word occurs. We will learn about these representations in [Part III](../part_iii/embeddings.ipynb).\n",
    "\n",
    " - `attribute_ruler` applies user-defined rules to *Tokens*, such as matches for a given linguistic pattern, and adds this information to the *Token* as an attribute if requested. We will explore the use of matchers in [Part III](../part_iii/pattern_matching.ipynb).\n",
    "\n",
    "Note also that the list of components under `nlp.pipeline` does not include a *Tokenizer*, because all texts must be tokenized for any kind of processing to take place. Hence the *Tokenizer* is placed under the `tokenizer` attribute of a *Language* object rather than the `pipeline` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that all pipeline components come with a computational cost. \n",
    "\n",
    "If you do not need the output, you should not include a component in the pipeline, because the time needed to process the data will be longer.\n",
    "\n",
    "To exclude a component from the pipeline, provide the `exclude` argument with a *string* or a *list* that contain the names of the components to exclude when initialising a *Language* object using the `load()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small language model for English, but exclude named entity\n",
    "# recognition ('ner') and syntactic dependency parsing ('parser'). \n",
    "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the `pipeline` attribute again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x12d3b8a40>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x13acba180>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x13d042d40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x13d034640>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the active components under the Language object 'nlp'\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output shows, the `ner` and `parser` components are no longer included in the pipeline.\n",
    "\n",
    "A *Language* object also provides a `analyze_pipes()` method for an overview of the pipeline components and their interactions. By setting the attribute `pretty` to `True`, spaCy prints out a table that lists the components and the annotations they produce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns       Requires   Scores      Retokenizes\n",
      "-   ---------------   -----------   --------   ---------   -----------\n",
      "0   tok2vec           doc.tensor                           False      \n",
      "                                                                      \n",
      "1   tagger            token.tag                tag_acc     False      \n",
      "                                                                      \n",
      "2   attribute_ruler                                        False      \n",
      "                                                                      \n",
      "3   lemmatizer        token.lemma              lemma_acc   False      \n",
      "\n",
      "\u001b[38;5;2mâœ” No problems found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Analyse the pipeline and store the analysis under 'pipe_analysis'\n",
    "pipe_analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `analyze_pipes()` method returns a Python *dictionary*, which contains the same information presented in the table above.\n",
    "\n",
    "You can use this dictionary to check that no problems are found before processing large volumes of data.\n",
    "\n",
    "Problem reports are stored in a dictionary under the key `problems`.\n",
    "\n",
    "We can access the values under the `problems` key by placing the name of the key in brackets `[ ]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok2vec': [], 'tagger': [], 'attribute_ruler': [], 'lemmatizer': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the value stored under the key 'problems'\n",
    "pipe_analysis['problems']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a dictionary with component names as keys, whose values contain lists of problems.\n",
    "\n",
    "In this case, the lists are empty, because no problems exist.\n",
    "\n",
    "We can, however, easily write a piece of code that checks if this is indeed the case.\n",
    "\n",
    "To do so, we loop over the `pipe_analysis` dictionary, using the `items()` method to fetch the key/value pairs.\n",
    "\n",
    "We assign the keys and values to variables `component_name` and `problem_list`, respectively.\n",
    "\n",
    "We then use the `assert` statement with the `len()` function and the *comparison operator* `==` to check that the length of the list is 0.\n",
    "\n",
    "If this assertion is not true, that is, if the length of `problem_list` is more than 0, which would indicate the presence of a problem, Python will raise an `AssertionError` and stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the key/value pairs in the dictionary. Assign the key and\n",
    "# value pairs to the variables 'component_name' and 'problem_list'.\n",
    "for component_name, problem_list in pipe_analysis['problems'].items():\n",
    "    \n",
    "    # Use the assert statement to check the list of problems; raise Error if necessary.\n",
    "    assert len(problem_list) == 0, f\"There is a problem with {component_name}: {problem_list}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we also print an error message using a *formatted string*. The error message is separated from the assertion by a comma. \n",
    "\n",
    "Note that the quotation marks are preceded by the character `f`. By declaring that this string can be formatted, we can insert variables into the string! \n",
    "\n",
    "The variable names inserted into the string are surrounded by curly brackets `{}`. If an error message is raised, these parts of the string will be populated using the values currently stored under the variables `component_name` and `problem_list`.\n",
    "\n",
    "If no problems are encountered, the loop will pass silently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing texts efficiently\n",
    "\n",
    "When working with larger volumes of data, processing the data as efficiently as possible is highly desirable.\n",
    "\n",
    "To illustrate the best practices of processing texts efficiently using spaCy, let's define a toy example that consists of a Python list with three example sentences from English Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On October 1, 2009, the Obama administration went ahead with a Bush administration program, increasing nuclear weapons production.',\n",
       " \"The 'Complex Modernization' initiative expanded two existing nuclear sites to produce new bomb parts.\",\n",
       " 'The administration built new plutonium pits at the Los Alamos lab in New Mexico and expanded enriched uranium processing at the Y-12 facility in Oak Ridge, Tennessee.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the language model again, because we need dependency\n",
    "# parsing for the following sections.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a list of example sentences\n",
    "sents = [\"On October 1, 2009, the Obama administration went ahead with a Bush administration program, increasing nuclear weapons production.\", \n",
    "         \"The 'Complex Modernization' initiative expanded two existing nuclear sites to produce new bomb parts.\", \n",
    "         \"The administration built new plutonium pits at the Los Alamos lab in New Mexico and expanded enriched uranium processing at the Y-12 facility in Oak Ridge, Tennessee.\"]\n",
    "\n",
    "# Call the variable to examine output\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list containing three sentences.\n",
    "\n",
    "spaCy _Language_ objects have a specific method, `pipe()`, for processing texts stored in a Python list.\n",
    "\n",
    "The `pipe()` method has been optimised for this purpose, processing texts in _batches_ rather than as individual items, which makes this method faster than processing each list item in a traditional `for` loop.\n",
    "\n",
    "The `pipe()` method takes a _list_ as input and returns a Python _generator_ named `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Language.pipe at 0x13d16f190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed the list of sentences to the pipe() method\n",
    "docs = nlp.pipe(sents)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators are Python objects that contain other objects. When called, a generator object will yield objects contained within. \n",
    "\n",
    "To retrieve all objects in a generator, we must cast the output into another object type, such as a list. \n",
    "\n",
    "You can think of the list as a data structure that is able to collect the generator output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[On October 1, 2009, the Obama administration went ahead with a Bush administration program, increasing nuclear weapons production.,\n",
       " The 'Complex Modernization' initiative expanded two existing nuclear sites to produce new bomb parts.,\n",
       " The administration built new plutonium pits at the Los Alamos lab in New Mexico and expanded enriched uranium processing at the Y-12 facility in Oak Ridge, Tennessee.]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the pipe generator into a list\n",
    "docs = list(docs)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a list of spaCy _Doc_ objects for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the output\n",
    "\n",
    "### Merging noun phrases\n",
    "\n",
    "The [previous section](basic_nlp.ipynb) described how tasks such as part-of-speech tagging and dependency parsing involve making predictions about individual _Tokens_ and their properties, such as their part-of-speech tags or syntactic dependencies.\n",
    "\n",
    "Occasionally, however, it may be more beneficial to operate with larger linguistic units instead of individual *Tokens*, such as noun phrases that consist of multiple _Tokens_.\n",
    "\n",
    "spaCy provides access to noun phrases via the attribute `noun_chunks` of a _Doc_ object.\n",
    "\n",
    "Let's print out the noun chunks in each _Doc_ object contained in the list `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October\n",
      "the Obama administration\n",
      "a Bush administration program\n",
      "nuclear weapons production\n",
      "The 'Complex Modernization' initiative\n",
      "two existing nuclear sites\n",
      "new bomb parts\n",
      "The administration\n",
      "new plutonium pits\n",
      "the Los Alamos lab\n",
      "New Mexico\n",
      "enriched uranium processing\n",
      "the Y-12 facility\n",
      "Oak Ridge\n",
      "Tennessee\n"
     ]
    }
   ],
   "source": [
    "# Define the first for-loop over the list 'docs'\n",
    "# The variable 'doc' refers to items in the list\n",
    "for doc in docs:\n",
    "    \n",
    "    # Loop over each noun chunk in the Doc object\n",
    "    for noun_chunk in doc.noun_chunks:\n",
    "        \n",
    "        # Print noun chunk\n",
    "        print(noun_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two `for` loops return several noun phrases that consist of multiple _Tokens_.\n",
    "\n",
    "For merging noun phrases into a single _Token_, spaCy provides a function named `merge_noun_tokens` that can be added to the pipeline stored in a _Language_ object using the `add_pipe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add component that merges noun phrases into single Tokens\n",
    "nlp.add_pipe('merge_noun_chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we do not need to reassign the _Language_ object under `nlp` to the same variable to update its contents. The `add_pipe` method adds the component to the _Language_ object automatically. \n",
    "\n",
    "We can verify that the component was added successfully by examining the `pipeline` attribute under the _Language_ object `nlp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x12d3a3b80>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x13e063400>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x13afdce20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x13d2834c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x13e09ecc0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x13e0afa80>),\n",
       " ('merge_noun_chunks',\n",
       "  <function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the pipeline components\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the final tuple in the list consists of the `merge_noun_chunks` function.\n",
    "\n",
    "To examine the consequences of adding this function to the pipeline, let's process the three example sentences again using the `pipe()` method of the _Language_ object `nlp`.\n",
    "\n",
    "We overwrite the previous results stored under the same variable by assigning the output to the variable `docs`.\n",
    "\n",
    "Note that we also cast the result into a list by wrapping the _Language_ object and the `pipe()` method into a `list()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[On October 1, 2009, the Obama administration went ahead with a Bush administration program, increasing nuclear weapons production.,\n",
       " The 'Complex Modernization' initiative expanded two existing nuclear sites to produce new bomb parts.,\n",
       " The administration built new plutonium pits at the Los Alamos lab in New Mexico and expanded enriched uranium processing at the Y-12 facility in Oak Ridge, Tennessee.]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the Language object 'nlp' to the list of sentences under 'sents'\n",
    "docs = list(nlp.pipe(sents))\n",
    "\n",
    "# Call the variable to examine the output\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superficially, everything remains the same: the list contains three _Doc_ objects. \n",
    "\n",
    "However, if we loop over the _Tokens_ in the first _Doc_ object in the list, which can be accessed using brackets at position zero, e.g. `[0]`, we can see that the noun phrases are now merged and tagged using the label `NOUN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On ADP\n",
      "October PROPN\n",
      "1 NUM\n",
      ", PUNCT\n",
      "2009 NUM\n",
      ", PUNCT\n",
      "the Obama administration NOUN\n",
      "went VERB\n",
      "ahead ADV\n",
      "with ADP\n",
      "a Bush administration program NOUN\n",
      ", PUNCT\n",
      "increasing VERB\n",
      "nuclear weapons production NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Loop over Tokens in the first Doc object in the list\n",
    "for token in docs[0]:\n",
    "    \n",
    "    # Print out the Token and its part-of-speech tag\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging noun phrases using the label `NOUN` is a reasonable approximation, as noun phrases typically take on similar grammatical functions as nouns.\n",
    "\n",
    "As rendering the syntactic parse using displacy shows, merging the noun phrases simplifies the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5a6bac695cf743089c66fc94334cc004-0\" class=\"displacy\" width=\"1975\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">On</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">October</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">1,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">2009,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the Obama administration</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">went</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">ahead</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">a Bush administration program,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">increasing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">nuclear weapons production.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-1\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210.0,354.0 L218.0,342.0 202.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-2\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,354.0 L393.0,342.0 377.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,354.0 L1273.0,342.0 1257.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-8\" stroke-width=\"2px\" d=\"M945,352.0 C945,89.5 1620.0,89.5 1620.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,354.0 L1628.0,342.0 1612.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a6bac695cf743089c66fc94334cc004-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a6bac695cf743089c66fc94334cc004-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1785.0,354.0 L1793.0,342.0 1777.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(docs[0], style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the noun phrases are now represented by single *Tokens*, the noun chunks are still available under the `noun_chunks` attribute of the *Doc* object.\n",
    "\n",
    "As shown below, spaCy stores noun chunks as *Spans*, whose `start` attribute determines the index of the Token where the _Span_ starts, while the `end` attribute determines where the _Span_ has ended.\n",
    "\n",
    "This information is useful, if the syntactic analysis reveals patterns that warrant a closer examination of the noun chunks and their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October <class 'spacy.tokens.span.Span'> 1 2\n",
      "the Obama administration <class 'spacy.tokens.span.Span'> 6 7\n",
      "a Bush administration program <class 'spacy.tokens.span.Span'> 10 11\n",
      "nuclear weapons production <class 'spacy.tokens.span.Span'> 13 14\n"
     ]
    }
   ],
   "source": [
    "# Loop over the noun chunks in the first Doc object [0] in the list 'docs'\n",
    "for noun_chunk in docs[0].noun_chunks:\n",
    "    \n",
    "    # Print out noun chunk, its type, the Token where the chunks starts and where it ends\n",
    "    print(noun_chunk, type(noun_chunk), noun_chunk.start, noun_chunk.end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging named entities\n",
    "\n",
    "Named entities can be merged in the same way as noun phrases by providing `merge_entities` to the `add_pipe()` method of the *Language* object.\n",
    "\n",
    "Let's start by removing the `merge_noun_chunks` function from the pipeline.\n",
    "\n",
    "The `remove_pipe()` method can be used to remove a pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'merge_noun_chunks' function from the pipeline under 'nlp'\n",
    "nlp.remove_pipe('merge_noun_chunks')\n",
    "\n",
    "# Process the original sentences again\n",
    "docs = list(nlp.pipe(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method returns a tuple containing the name of the removed component (in this case, a function) and the component itself.\n",
    "\n",
    "We can verify this by calling the `pipeline` attribute of the _Language_ object `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x12d3a3b80>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x13e063400>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x13afdce20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x13d2834c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x13e09ecc0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x13e0afa80>)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add the `merge_entities` component to the pipeline under `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'merge_entities' function to the pipeline\n",
    "nlp.add_pipe('merge_entities')\n",
    "\n",
    "# Process the data again\n",
    "docs = list(nlp.pipe(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the result by looping over the _Tokens_ in the third _Doc_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET\n",
      "administration NOUN\n",
      "built VERB\n",
      "new ADJ\n",
      "plutonium NOUN\n",
      "pits NOUN\n",
      "at ADP\n",
      "the DET\n",
      "Los Alamos PROPN\n",
      "lab NOUN\n",
      "in ADP\n",
      "New Mexico PROPN\n",
      "and CCONJ\n",
      "expanded VERB\n",
      "enriched ADJ\n",
      "uranium NOUN\n",
      "processing NOUN\n",
      "at ADP\n",
      "the DET\n",
      "Y-12 PROPN\n",
      "facility NOUN\n",
      "in ADP\n",
      "Oak Ridge PROPN\n",
      ", PUNCT\n",
      "Tennessee PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Loop over Tokens in the third Doc object in the list\n",
    "for token in docs[2]:\n",
    "    \n",
    "    # Print out the Token and its part-of-speech tag\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entities that consist of multiple *Tokens*, as exemplified by place names such as \"Los Alamos\" and \"New Mexico\", have been merged into single *Tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should have given you an idea of how the spaCy pipeline can be modified for efficient processing.\n",
    "\n",
    "The [following section](evaluating_nlp.ipynb) introduces you to evaluating the performance of language models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
