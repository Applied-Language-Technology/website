{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing textual data using pandas\n",
    "\n",
    "This lecture introduces you to preparing and managing textual data for analysis using *[pandas](http://pandas.pydata.org/)*, a library for processing and analysing table-like data.\n",
    "\n",
    "Typically, you must load and prepare the data yourself, either from a single file or from multiple files.\n",
    "\n",
    "Typical formats for distributing corpora include CSV files, which stands for Comma-separated Values, and JSON, which stands for JavaScript Object Notation or simple plain text files.\n",
    "\n",
    "After this lecture, you should:\n",
    "\n",
    "- have a basic understanding about *pandas*, a Python library for processing and analysing data\n",
    "- how to import data into a *pandas* DataFrame\n",
    "- how to explore data stored in a *pandas* DataFrame\n",
    "- how to append data to a *pandas* DataFrame\n",
    "- how to save the data in a *pandas* DataFrame\n",
    "\n",
    "The following example shows how to load a corpus from a CSV file for processing in Python using the [SFU Opinion and Comments Corpus (SOCC)](https://github.com/sfu-discourse-lab/SOCC).\n",
    "\n",
    "Let's load a part of the SFU Opinion and Comments Corpus, which contains the opinion articles from [The Globe and Mail](https://www.theglobeandmail.com/), a Canadian newspaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data to *pandas*\n",
    "\n",
    "Let's start by importing the *pandas* library. Note that we can control the name of the imported module using the `as` addition to the `import` command. pandas is commonly abbreviated `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data from a single file\n",
    "\n",
    "If you get lucky, your data might be contained in a single file in a structured format, such as comma-separated values (CSV).\n",
    "\n",
    "*pandas* provides plenty of functions for [reading data in various formats](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html). You can even try importing Excel sheets – although I would not recommend this.\n",
    "\n",
    "For current purposes, after importing *pandas* as `pd`, we can use the `read_csv()` functions to read files with comma-separated values, such as the SOCC corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc = pd.read_csv('00_data/socc_gnm_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* does all the heavy lifting for us and returns the contents of the CSV file in a *pandas* DataFrame, which is data structure native to *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(socc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `head()` method of a DataFrame to check out the first five rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is organised into a tabular form: a DataFrame, the fundamental data structure in *pandas*.\n",
    "\n",
    "The DataFrame contains several columns such as **article_id**, **title** and **article_text**, accompanied by an index for each row (**0, 1, 2, 3, 4**).\n",
    "\n",
    "The `.at[]` accessor can be used to inspect a single item in the DataFrame.\n",
    "\n",
    "Let's examine the value in the column **title** at index 123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc.at[123, 'title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data from multiple files\n",
    "\n",
    "Another common scenario is that you have multiple files with text data, which you want to load into *pandas*.\n",
    "\n",
    "Let's first collect the files that we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the patch library\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a Path to the corpus directory\n",
    "corpus_dir = Path('00_data')\n",
    "\n",
    "# Get all .txt files in the corpus directory\n",
    "corpus_files = list(corpus_dir.glob('*.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accommodate our data, let's create an empty pandas DataFrame and specify its shape in advance, that is, the number of rows (`index`) and the names of the columns `columns`.\n",
    "\n",
    "We can easily get the number of rows needed using Python's `range()` function, which generates a list of numbers that fall within certain range.\n",
    "\n",
    "Here we start from zero and finish when we get up to the number of files in the corpus.\n",
    "\n",
    "For columns, we simply create columns for holding the name of the file and its actual contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=range(0, len(corpus_files)), columns=['filename', 'text'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an empty data with rows for each file in the corpus, we can loop over the files and add their contents to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the corpus files\n",
    "for i, f in enumerate(corpus_files):\n",
    "    \n",
    "    # Open the file for reading\n",
    "    c_file = open(f, encoding=\"utf-8\")\n",
    "    \n",
    "    # Get the filename from the Path object\n",
    "    filename = f.name\n",
    "        \n",
    "    # Read the file contents\n",
    "    text = c_file.read()\n",
    "    \n",
    "    # Assign the text from the file to index 'i' at column 'text'\n",
    "    # using the .at accessor – note that this modifies the DataFrame\n",
    "    # \"in place\" – you don't need to assign the result into a variable\n",
    "    df.at[i, 'text'] = text\n",
    "    \n",
    "    # We then do the same to the filename\n",
    "    df.at[i, 'filename'] = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick in-class exercise\n",
    "\n",
    "Let's go back to the SOCC corpus stored under `socc`.\n",
    "\n",
    "Who is the author (`author`) of article at index 256? \n",
    "\n",
    "How many top-level comments (`ntop_level_comments`) did the article at index 1000 receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code below and press Shift+Enter to run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* also provides various methods for examining the contents of entire columns, which can be accessed just like the keys and values of a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the column `author` contains 10399 objects. \n",
    "\n",
    "The numbers on the left-hand side give the index: you can think of them as row numbers.\n",
    "\n",
    "We can use built-in methods such as `value_counts()` to easily count the number of unique authors in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc['author'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the editorial team is responsible for most of the editorials. Who would have guessed?\n",
    "\n",
    "Let's take another look at the data by visualising the result by calling the `.plot()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is some Jupyter magic that allows us to render matplotlib plots in the notebooks!\n",
    "# You only need to enter this command once.\n",
    "%matplotlib inline\n",
    "\n",
    "# Count the values in the column 'author' and clip the result to top-10 before plotting.\n",
    "socc['author'].value_counts()[:10].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with numerical values, we can also use the `.describe()` method to get basic descriptive statistics on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc['ntop_level_comments'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the column `ntop_level_comments` has a total of 10339 rows. \n",
    "\n",
    "The average number of comments received by an editorial is approximately 26, but this number fluctuates, as the standard deviation from the mean is nearly 40.\n",
    "\n",
    "- Some editorials do not have any comments at all, as indicated by the minimum value of 0. \n",
    "- The lowest quartile shows that 25% of the data has only one comment or less (none).\n",
    "- The second quartile (50%), which is also known as the median, indicates that *half* of the data has less than 14 comments and *half* has more than 14 comments. \n",
    "- The third quartile shows that 75% of the data has 35 comments or less. \n",
    "- The most commented editorial has 1378 comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we would like to find the articles that received no comments at all?\n",
    "\n",
    "For this purpose, we can use the *pandas* accessor `.loc` to access columns or rows based on their labels.\n",
    "\n",
    "We know that the number of comments is stored in the column `ntop_level_comments`, but we also need to tell *pandas* which DataFrame contains this column (`socc`). This causes the somewhat repetitive expression in the beginning.\n",
    "\n",
    "Next, we need to provide a command for \"is equal to\". Since the single equal sign `=` is reserved for assigning variables, two equal signs are used for comparison.\n",
    "\n",
    "Finally, we place the value we want to evaluate against on the right-hand side of the double equal sign `==`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc.loc[socc['ntop_level_comments'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple criteria using the `&` symbol, which is the Python operator for \"AND\".\n",
    "\n",
    "Note that individual criteria must be placed in parentheses `()` to perform the operation.\n",
    "\n",
    "Let's check if the first author in our result, Hayden King, wrote any other articles with zero comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc.loc[(socc['ntop_level_comments'] == 0) & (socc['author'] == 'Hayden King')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick in-class exercise\n",
    "\n",
    "How many articles with zero top-level comments were authored by the editorial team (`GLOBE EDITORIAL`)?\n",
    "\n",
    "Write out the whole command yourself instead of copy-pasting to get an idea of the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code below and press Shift+Enter to run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding information to pandas DataFrames\n",
    "\n",
    "You can easily add information to *pandas* DataFrames.\n",
    "\n",
    "One common scenario could involve loading some data from an external file (such as a CSV or JSON file), performing some analyses and storing the results to the same DataFrame.\n",
    "\n",
    "We can easily add an empty column to the DataFrame. This is achieved using the column accessor `[]` and the Python datatype `None`.\n",
    "\n",
    "Let's add a new column named `r_comments` to the DataFrame `socc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc['r_comments'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's populate the column with some data by calculating how many percent of the comments are top-level comments.\n",
    "\n",
    "We can assume that a high percentage of top-level comments indicates comments about the article, whereas a lower percentage indicates more discussion about the comments posted.\n",
    "\n",
    "To get the proportion of top-level comments out of all comments, we must divide the number of top-level comments by the number of all comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc['r_comments'] = socc['ntop_level_comments'] / socc['ncomments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *pandas* column accessors can be used very flexibly to access and manipulate (in this case, divide) the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the column `r_comments` now stores the result of our calculation!\n",
    "\n",
    "However, we should also keep in mind that some articles did not receive any comments at all: thus we would have divided zero by zero.\n",
    "\n",
    "Let's take a look at these cases at the tail of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result is marked as `NaN` or \"not a number\".\n",
    "\n",
    "This indicates that the operation was performed on these cells as well, but the result was not a number.\n",
    "\n",
    "*pandas* automatically ignores these values when performing calculations, as show by the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc['r_comments'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference in the result for the count. Only 7797 items out of 10399 were included in the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we would like to do some NLP and store the results in the DataFrame?\n",
    "\n",
    "Instead of processing the whole DataFrame, let's select articles that seem to cause discussion among commenters and have more than 200 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk = socc.loc[(socc['r_comments'] <= 0.384) & (socc['ncomments'] >= 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import spacy and load the language model for English and assign it to the variable `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')  # Note that we now load a medium-sized language model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we want to process the article titles, let's break down the process into small steps and create a placeholder column named `title_nlp` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk['title_nlp'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* warns about performing this command, because `talk` is only a slice or a _view_ into the DataFrame. \n",
    "\n",
    "Assigning a new column to **only a part of the DataFrame** would cause problems by breaking the tabular structure.\n",
    "\n",
    "We can fix the situation by creating a _deep copy_ of the slice using Python's `.copy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk = talk.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating an empty column again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk['title_nlp'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding each title for our NLP pipeline is not as straightforward as dividing one pandas column full numbers with another of the same type.\n",
    "\n",
    "To take each individual article title and feed it to spaCy for processing, we need to use the pandas `.apply()` method with a `lambda` function.\n",
    "\n",
    "Python's `lambda` functions are essentially functions that are defined on the fly.\n",
    "\n",
    "The `lambda` function is followed by a variable, in this case `title_text` and a colon `:`.\n",
    "\n",
    "What `lambda` essentially does is somewhat equivalent to \"take whatever is given and do something with it\".\n",
    "\n",
    "The part on the right-hand side of the colon `:` tells Python to take the column contents provided by `.apply()` and feed them to spaCy for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk['title_nlp'] = talk['title'].apply(lambda title_text: nlp(title_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the processed titles in a separate column named `title_nlp`!\n",
    "\n",
    "Let's take a closer look at the first row, whose index is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk.at[2, 'title_nlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(talk.at[2, 'title_nlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the cell contains a spaCy _Doc_ object.\n",
    "\n",
    "Let's now define our own Python **function** to fetch lemmas for each noun in the title.\n",
    "\n",
    "Python functions are defined using the command `def`, which is followed by the name of the function, in this case `get_nouns`. The input to the function is given in parentheses that follow the name of the function.\n",
    "\n",
    "In this case, we name a variable for the input called `nlp_text`. This is an arbitrary variable, which is needed for referring to whatever is being provided as input to the function. To put it simply, you can think of this variable as the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(nlp_text):\n",
    "    \n",
    "    # First we make sure that the input is of correct type\n",
    "    assert type(nlp_text) == spacy.tokens.doc.Doc\n",
    "    \n",
    "    # Let's set up a placeholder list for our lemmas\n",
    "    lemmas = []\n",
    "    \n",
    "    # We begin then begin looping over the Doc object\n",
    "    for token in nlp_text:\n",
    "        \n",
    "        # If the fine-grained POS tag for the token is a noun (NN)\n",
    "        if token.tag_ == 'NN':\n",
    "            \n",
    "            # Append the token lemma to the list of lemmas\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # When the loop is complete (note the indentation), \n",
    "    # join the list of items together and return a string\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our function, let's use it to collect all nouns to the column `nouns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk['nouns'] = talk['title_nlp'].apply(lambda text: get_nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily extract information from pandas DataFrames into Python's native data structures. \n",
    "\n",
    "The `tolist()` method, for instance, can be used to extract the column contents into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = talk['nouns'].tolist()\n",
    "\n",
    "noun_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now is a list of lists. Let's loop over the list and collect the items into a single list named `final_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the placeholder list\n",
    "final_list = []\n",
    "\n",
    "# Loop over each list in the list of lists\n",
    "for nlist in noun_list:\n",
    "    \n",
    "    # Extend the final list with the current list\n",
    "    final_list.extend(nlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly examine the first ten items in final list and then count the number of items in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily count the number of unique items in the list using a Python *Counter* object and its `most_common()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(final_list).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Is there a connection between editorial topics and the ensuing discussion among readers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results\n",
    "\n",
    "*pandas* DataFrames can be easily saved as *pickled* objects using the `to_pickle()` method.\n",
    "\n",
    "The `to_pickle()` method takes a string as input, which defines a path to the file in which the DataFrame should be written.\n",
    "\n",
    "Let's pickle the DataFrame with the three articles stored under `df` into a file named `pickled_df.pkl` into the directory `00_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('00_data/pickled_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily check if the data has been saved successfully by reading the file contents using the `read_pickle()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_pickle('00_data/pickled_df.pkl')\n",
    "\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the DataFrames, which returns a Boolean value (True/False) for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df == df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "This lecture should have given you a brief introduction to *pandas* and its basic data structure, a DataFrame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
